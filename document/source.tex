\documentclass[oneside,openright,12pt,final,en]{mgr}


\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british,UKenglish,USenglish,english,american]{babel}
\usepackage{showlabels}
\usepackage{float}
\usepackage{listings}
\usepackage{siunitx}
\usepackage{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{dred}{rgb}{0.545,0,0}
\definecolor{dblue}{rgb}{0,0,0.545}
\definecolor{lgrey}{rgb}{0.9,0.9,0.9}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{bluekeywords}{rgb}{0,0,1}
\definecolor{eminence}{RGB}{0,174,178}
\lstdefinelanguage{cpp}{
	backgroundcolor=\color{lgrey},  
	basicstyle=\footnotesize \ttfamily \color{black} \bfseries,   
	breakatwhitespace=false,       
	breaklines=true,               
	captionpos=b,                   
	commentstyle=\color{dkgreen},   
	deletekeywords={...},          
	escapeinside={\%*}{*)},                  
	frame=single,                  
	language=C++,                
	keywordstyle=\color{bluekeywords},  
	morekeywords={BRIEFDescriptorConfig,string,TiXmlNode,DetectorDescriptorConfigContainer,istringstream,cerr,exit}, 
	identifierstyle=\color{black},
	emph={BigInteger,cudaStream_t,dim3,DeviceWrapper},
	emphstyle={\color{eminence}},
	stringstyle=\color{blue},      
	numbers=right,                 
	numbersep=5pt,                  
	numberstyle=\tiny\color{black}, 
	rulecolor=\color{black},        
	showspaces=false,               
	showstringspaces=false,        
	showtabs=false,                
	stepnumber=1,                   
	tabsize=5,                     
	title=\lstname,                 
}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{caption}

\title{ }
\engtitle{Timing Attack Resistant Implementation of RSA on GPU.}
\author{Krzysztof Hamerski}
\supervisor{dr Maciej GÄ™bala}
\field{Computer Science}
\specialisation{Computer Security}


\begin{document}
\bibliographystyle{plabbrv}

\maketitle 
\tableofcontents

\chapter{Introduction}

Public key cryptography is the key factor in providing secure communication between two parties. Fast development of distributed system requiring not only security, but also integrity and non-repudiation has pushed cryptography to the limit. Since 1978\cite{rsa} most commonly used cryptosystem is RSA, which provides asymmetric encryption, as well as generation of digital signatures. The security of RSA is mainly base on the bitwise key length. As computational power of modern CPUs arises, the minimal bit length of RSA key gets significantly bigger to provide sufficient security. At least 4096 bits long keys are considered secure nowadays. This leads to very high workload required to perform encryption/decryption. RSA is mainly based on modular arithmetics and simple computations become infeasible for moder CPUs, when dealing with so large integers. 

One of the solution to this problem is to parallelize. GPGPU\cite{gpgpu} (for General-Purpose
computing on the Graphics Processing Unit) enables the use of GPU for parallel computation other than graphics. GPUs are designed to perform computations in parallel. Since every PC is equipped with some kind of GPU, one can easily exploit its capabilities.    
NVIDIA has made it even more accessible by creating CUDA (Compute Unified Device Architecture)\cite{cuda}. It is a parallel computing platform and API, which exposes GPUs true potential. 
  
\section{Environment Specification}
This project was developed in Microsoft Visual Studio 2015\cite{vs} with NSight plugin and CUDA API.\cite{cuda} Source code is written mainly in C++ and inline assembly - PTX.\cite{ptx} Table \ref{dspec} more precisely illustrates GPU specification on which the program and all tests are run.

\begin{table}[!h]
	\caption{Device specification}
	\label{dspec}
	\centering
	\begin{tabular}{ | l | l |}
		\hline
		Device name: & GeForce GTX 960 \\ \hline
		CUDA Driver Version: & 9.0 \\
		CUDA Runtime Version: & 8.0  \\
		CUDA Capability version number: & 5.2  \\ 
		Total amount of global memory: & 4096 MBytes (4294967296 bytes)  \\
    	GPU Max Clock rate:  & 1304 MHz (1.30 GHz)  \\
		Total amount of shared memory per block: & 49152 bytes  \\
		Warp size: & 32  \\\hline
	\end{tabular}
\end{table}
 
 Table below presents full platform and system information.
 
 \begin{table}[h]
 	\caption{PC specification}
 	\label{pspec}
 	\centering
 	\begin{tabular}{ | l | l |}
 		\hline
 		Operating System: & Windows 10 Education 64-bit \\
 		Motherboard: & Gigabyte Technology Co., Ltd. P55A-UD4 \\
 		CPU: & Intel(R) Core(TM) i5 CPU 750  @ 2.67GHz (4 CPUs), ~2.7GHz  \\
 		RAM Memory: : & 4096MB \\\hline
 	\end{tabular}
 \end{table}


\chapter{Theory}
This chapter basically presents minimal amount of theory required to fully understand the concepts, problems and solutions which were applied and implemented within this project.

\section{RSA}

The RSA algorithm was created by Ronald Rivest, Adi Shamir, and Leonard Adleman in 1970s.
The security of the algorithm is based on the problem of integer factorization.

\begin{itemize}
	\item Key generation\cite{rsad}
	\begin{itemize}
		\item Choose two large and distinct prime numbers $p$ and $q$.
		\item Compute the modulus $n$
		\[n=pq\]
		\item Compute 
		\[\lambda(n) = lcm(\lambda(p), \lambda(q)) = lcm(p-1, q-1)\]
		where $\lambda$ is the Carmichael Totient Function\cite{lambda}. This value must be kept in private.
		\item Choose an integer $e$ such that:
		\[1 < e < \lambda(n)\] 
		and
		\[gcd(e, \lambda(n)) = 1\]
		This means $e$ and $\lambda(n)$ are coprime.\cite{primes}
		\item Choose value $d$ for decryption and solve for $d$:
		\[de \equiv 1 (mod \lambda(n))\] 
		$e$ is used for encryption. Usually this is done the other way around. $e$ is chosen first so it has short bit length and small Hamming weight,\cite{hamm} which provides for much faster encryption. In most cases $e=3$,$5$ or $7$. High security is provided if larger number is used. Most common is Fermat's four: $e=2^{16}+1=65537 = 0$x$10001$
		\item Public key is then:
		\[(e,n)\]
		and the private key:
		\[(d,n)\]
	\end{itemize}
	\item Encryption
	
	For message $m$ the ciphertext $c$ is in relation:
	\begin{center}
	$c\equiv m^e$ mod$n$
	\end{center}
	\item Decryption
	\begin{center}
		$m\equiv c^d$ mod$n$ $\equiv(m^e)^d$ mod$n$ $\equiv m$ mod$n$
	\end{center}
	
\end{itemize}

\section{Side channel attacks}

Side-channel attacks are very powerful attacks against cryptographic implementations. They take the advantage of implementation flaws, gathering informations about almost everything that can be measured during execution time. If no care is taken, side-channel attacks can be used to compromise virtually any mathematically secure system. 
\begin{figure}[H]
	\centering
	\includegraphics{sidechannel}
	\caption{An example of cryptosystem including side-channel information leakage}
	\label{fig:sidechannel}
\end{figure}

Side-channel attacks aim to retrieve secret data from a cryptographic system by observing factors outside the normal computation. Power consumption, execution time and even electromagnetic radiation, sound, light and heat can leak some information about data being computed. Running statistical analysis on this partial information makes even the most sophisticated systems vulnerable to breakage. 


\subsection{Timing attacks}

Implementations of cryptographic algorithms often perform computations in non-constant
time, due to performance optimizations. If such operations involve secret parameters, these timing
variations can leak some information and, provided enough knowledge of the implementation is at
hand, a careful statistical analysis could even lead to the total recovery of these secret parameters.\cite{sc} 

\section{Parallel programming on CUDA}

\subsection{Shared Memory}
"Threads within a block can cooperate by sharing data through shared memory and by synchronizing their execution to coordinate memory accesses. Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory. For that shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it. A typical programming pattern is to stage data coming from device memory into shared memory, process the data in shared memory while sharing the shared view of the data across the threads of a block, and write the results back to device memory.

To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously. Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module. However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized. The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. If the number of separate memory requests is n, the initial memory request is said to cause n-way bank conflicts. To get maximum performance, it is therefore important to minimize bank conflicts.

Shared memory has 32 banks that are organized such that successive 32bit words map to successive banks. A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32bit word (even though the two addresses fall in the same bank).

For devices of compute capability 3.x, shared memory has 32 banks with two addressing modes that can be configured using cudaDeviceSetSharedMemConfig(). Each bank has a bandwidth of 64 bits per clock cycle. In 64bit mode, successive 64bit words map to successive banks. A shared memory request for a warp does not generate a bank conflict between two threads that access any sub-word within the same 64bit word (even though the addresses of the two sub-words fall in the same bank). In 32bit mode (default), successive 32bit words map to successive banks. A shared memory request for a warp does not generate a bank conflict between two threads that access any sub-word within the same 32bit word or within two 32bit words whose indices i and j are in the same 64word aligned segment (i.e., a segment whose first index is a multiple of 64) and such that j=i+32 (even though the addresses of the two sub-words fall in the same bank)."\cite{nsight}

\chapter{Implementation}

The code was written in C++ language. In order to minimize data movement between host and the device, most of logics and computation are executed fully on GPU. Implementation of RSA encryption requires only one function - modular exponentiation of a multi precision integer. Implemented Big Integer class provides much more functions, to properly handle data and validate results. Code listing below shows the header of class BigInteger.

\begin{lstlisting}[language=cpp,caption={BigInteger.h}]
class BigInteger
{
//fields
public:

// 4096 bits
static const int ARRAY_SIZE = 128;	

private:
// Magnitude array in little endian order.
// Most-significant int is mag[length-1].
// Least-significant int is mag[0].
// Allocated on the device.
unsigned int* deviceMagnitude;

// same array allocated on the host 
// provides faster access if nothing was changed
unsigned int* hostMagnitude;

// flag indicating if hostMagnitude matches deviceMagnitude
bool upToDate;

// Device wrapper instance diffrent for every integer
// to provide parallel execution
DeviceWrapper* deviceWrapper;

// methods
public:
BigInteger();
BigInteger(const BigInteger& x);
BigInteger(unsigned int value);
~BigInteger();
const unsigned int& operator[](int index);

// factory
static BigInteger* fromHexString(const char* string);
static BigInteger* createRandom(int bitLength);

// setters, getters
void set(const BigInteger& x);	
unsigned int* getDeviceMagnitude(void) const;

// arithmetics
void add(const BigInteger& x);
void subtract(const BigInteger& x);
void multiply(const BigInteger& x);
void square(void);
void mod(const BigInteger& modulus);
void multiplyMod(const BigInteger& x, const BigInteger& modulus);
void squareMod(const BigInteger& modulus);
void powerMod(BigInteger& exponent, const BigInteger& modulus);

// logics
void shiftLeft(int bits);
void shiftRight(int bits);

// extras
bool equals(const BigInteger& value) const;
int compare(const BigInteger& value) const;
int getBitwiseLengthDiffrence(const BigInteger& value) const;
int getBitwiseLength(void) const;
int getLSB(void) const;
bool testBit(int bit);
void synchronize(void);
char* toHexString(void);
void print(const char* title);

//timer
void startTimer(void);
unsigned long long stopTimer(void);

// async calls
// must call synchronize to read from
void modAsync(const BigInteger& modulus);
void multiplyModAsync(const BigInteger& x, const BigInteger& modulus);
void squareModAsync(const BigInteger& modulus);

private:

void setMagnitude(const unsigned int* magnitude);
void clear(void);
void updateDeviceMagnitiude(void);
void updateHostMagnitiude(void);
static unsigned int random32(void);

/*
Parses hex string to unsigned int type.
Accepts both upper and lower case, no "0x" at the beginning.
E.g.: 314Da43F 
*/	
static unsigned int parseUnsignedInt(const char* hexString);
};

\end{lstlisting}

Big Integer class handles mathematical logic, validates input / output, and provides comfortable and readable interface.

The intermediary between Big Integer and GPU is another class - Device Wrapper.
It handles GPU kernel launches, synchronization and data movement. Class definition is listed below.

\begin{lstlisting}[language=cpp,caption={DeviceWrapper.h}]
class DeviceWrapper
{

private:

// main stream for kernel launches
cudaStream_t mainStream;

// lauch config
dim3 block_1, block_2, block_4;
dim3 thread_warp, thread_2_warp, thread_4_warp;

// 4 ints to help store results
int* deviceWords;

// auxiliary arrays
unsigned int* device4arrays;
unsigned int* device128arrays;
unsigned int* deviceArray;

unsigned long long* deviceStartTime;
unsigned long long* deviceStopTime;

public:

DeviceWrapper();
~DeviceWrapper();

// sync
unsigned int* init(int size) const;
unsigned int* init(int size, const unsigned int* initial) const;
void updateDevice(unsigned int* device_array, const unsigned int* host_array, int size) const;
void updateHost(unsigned int* host_array, const unsigned int* device_array, int size) const;
void free(unsigned int* device_x) const;

// extras
void clearParallel(unsigned int* device_x) const;
void cloneParallel(unsigned int* device_x, const unsigned int* device_y) const;
int compareParallel(const unsigned int* device_x, const unsigned int* device_y) const;
bool equalsParallel(const unsigned int* device_x, const unsigned int* device_y) const;
int getLSB(const unsigned int* device_x) const;
int getBitLength(const unsigned int* device_x) const;
void synchronize(void);

// measure time
void startClock(void);
unsigned long long stopClock(void);

// logics
void shiftLeftParallel(unsigned int* device_x, int bits) const;
void shiftRightParallel(unsigned int* device_x, int bits) const;

// arithmetics
void addParallel(unsigned int* device_x, const unsigned int* device_y) const;
void subtractParallel(unsigned int* device_x, const unsigned int* device_y) const;
void multiplyParallel(unsigned int* device_x, const unsigned int* device_y) const;
void squareParallel(unsigned int* device_x) const;
void squareParallelAsync(unsigned int* device_x) const;
void modParallel(unsigned int* device_x, unsigned int* device_m) const;
void modParallelAsync(unsigned int* device_x, unsigned int* device_m) const;
void multiplyModParallel(unsigned int* device_x, const unsigned int* device_y, const unsigned int* device_m) const;
void multiplyModParallelAsync(unsigned int* device_x, const unsigned int* device_y, const unsigned int* device_m) const;
void squareModParallel(unsigned int* device_x, const unsigned int* device_m) const;
void squareModParallelAsync(unsigned int* device_x, const unsigned int* device_m) const;

private:
void inline addParallelWithOverflow(unsigned int* device_x, const unsigned int* device_y, int blocks) const;
};
\end{lstlisting}

Project also contains Test class to validate computations, measure times and simulate encryption.
RSA class contains single "encrypt" function, which encrypts provided value. Full class diagram is presented on figure \ref{fig:classdiagram}.

\begin{figure}
	\centering
	\includegraphics{ClassDiagram}
	\caption{Class Diagram}
	\label{fig:classdiagram}
\end{figure}


\section{Parallel equals}
\section{Parallel compare}
\section{Parallel bit length}
\section{Parallel left shift}
\section{Parallel right shift}
\section{Parallel add}
\section{Parallel subtract}
\section{Parallel multiply}
\section{Parallel modulo reduction}
\section{Parallel multiply modulo}
\section{Parallel power modulo}

\chapter{Testing}
This chapter presents the behavior of implemented algorithms based on the inputs' lengths. All of the functions are designed to handle data up to 4096 bits long. Each test is repeated 50-100 times (depends on the complexity) to provide rather average results.  


Other test were executed using CUDA Profiler Activity. "This tool gathers detailed performance information, in addition to timing and launch configuration details. A CUDA Profiler activity consists of a kernel filter and a set of profiler experiments. Profile experiments are directed analysis tests targeted at collecting in-depth performance information for an isolated instance of a kernel launch."\cite{nsight}

\section{CUDA experiments}

\subsection{Occupancy}
"A warp (32 threads) is considered active from the time its threads begin executing to the time when all threads in the warp have exited from the kernel. There is a maximum number of warps which can be concurrently active on a Streaming Multiprocessor (SM). Occupancy is defined as the ratio of active warps on an SM to the maximum number of active warps supported by the SM. Occupancy varies over time as warps begin and end, and can be different for each SM.

Low occupancy results in poor instruction issue efficiency, because there are not enough eligible warps to hide latency between dependent instructions. When occupancy is at a sufficient level to hide latency, increasing it further may degrade performance due to the reduction in resources per thread. An early step of kernel performance analysis should be to check occupancy and observe the effects on kernel execution time when running at different occupancy levels.

\subsection{Theoretical occupancy}
There is an upper limit for active warps, and thus also for occupancy, derivable from the launch configuration, compile options for the kernel, and device capabilities. Each block of a kernel launch gets distributed to one of the SMs for execution. A block is considered active from the time its warps begin executing to the time when all warps in the block have exited from the kernel. The number of blocks which can execute concurrently on an SM is limited by the factors listed below. The upper limit for active warps is the product of the upper limit for active blocks and the number of warps per block. Thus, the upper limit for active warps can be raised by increasing the number of warps per block (defined by block dimensions), or by changing the factors limiting how many blocks can fit on an SM to allow more active blocks. The limiting factors are:

\begin{itemize}
	\item Warps per SM
		
	The SM has a maximum number of warps that can be active at once. Since occupancy is the ratio of active warps to maximum supported active warps, occupancy is $100\%$ if the number of active warps equals the maximum. If this factor is limiting active blocks, occupancy cannot be increased. For example, on a GPU that supports 64 active warps per SM, 8 active blocks with 256 threads per block (8 warps per block) results in 64 active warps, and $100\%$ theoretical occupancy. Similarly, 16 active blocks with 128 threads per block (4 warps per block) would also result in 64 active warps, and $100\%$ theoretical occupancy.	
	
	\item Blocks per SM
	
	The SM has a maximum number of blocks that can be active at once. If occupancy is below 100\% and this factor is limiting active blocks, it means each block does not contain enough warps to reach 100\% occupancy when the device's active block limit is reached. Occupancy can be increased by increasing block size. For example, on a GPU that supports 16 active blocks and 64 active warps per SM, blocks with 32 threads (1 warp per block) result in at most 16 active warps (25\% theoretical occupancy), because only 16 blocks can be active, and each block has only one warp. On this GPU, increasing block size to 4 warps per block makes it possible to achieve 100\% theoretical occupancy.
	
	\item Registers per SM
	
	The SM has a set of registers shared by all active threads. If this factor is limiting active blocks, it means the number of registers per thread allocated by the compiler can be reduced to increase occupancy. Kernel execution time and average eligible warps should be monitored carefully when adjusting registers per thread to control occupancy. The performance gain from improved latency hiding due to increased occupancy may be outweighed by the performance loss of having fewer registers per thread, and spilling to local memory more often. The best-performing balance of occupancy and registers per thread can be found experimentally by tracing the kernel compiled with different numbers of registers per thread.
	
	\item Shared memory per SM
	
	The SM has a fixed amount of shared memory shared by all active threads. If this factor is limiting active blocks, it means the shared memory needed per thread can be reduced to increase occupancy. Shared memory per thread is the sum of static shared memory, the total size needed for all '\_\_shared\_\_' variables, and dynamic shared memory, the amount of shared memory specified as a parameter to the kernel launch. For some CUDA devices, the amount of shared memory per SM is configurable, trading between shared memory size and L1 cache size. If such a GPU is configured to use more L1 cache and shared memory is the limiting factor for occupancy, then occupancy can also be increased by choosing to use less L1 cache and more shared memory.
\end{itemize}

\subsection{Achieved occupancy}
Theoretical occupancy shows the upper bound active warps on an SM, but the true number of active warps varies over the duration of the kernel, as warps begin and end. An SM contain one or more warp schedulers. Each warp scheduler attempts to issue instructions from a warp on each clock cycle. To sufficiently hide latencies between dependent instructions, each scheduler must have at least one warp eligible to issue an instruction every clock cycle. Maintaining as many active warps as possible (a high occupancy) throughout the execution of the kernel helps to avoid situations where all warps are stalled and no instructions are issued. Achieved occupancy is measured on each warp scheduler using hardware performance counters to count the number of active warps on that scheduler every clock cycle. These counts are then summed across all warp schedulers on each SM and divided by the clock cycles the SM is active to find the average active warps per SM. Dividing by the SM's maximum supported number of active warps gives the achieved occupancy per SM averaged over the duration of the kernel. Averaging across all SMs gives the overall achieved occupancy.

\subsection{Occupancy charts}
\begin{itemize}
	\item Varying Block Size
	
	Shows how varying the block size while holding other parameters constant would affect the theoretical occupancy. The circled point shows the current number of threads per block and the current upper limit of active warps. If the chart's line goes higher than the circle, changing the block size could increase occupancy without changing the other factors.
	\begin{figure}[h!]
		\centering
		\includegraphics{AchievedOccupancyVaryingBlockSize}
		\caption{Varying block size chart example}
	\end{figure}
	
	\item Varying Register Count
	
	Shows how varying the register count while holding other parameters constant would affect the theoretical occupancy. The circled point shows the current number of registers per thread and the current upper limit of active warps. If the chart's line goes higher than the circle, changing the number of registers per thread could increase occupancy without changing the other factors.
		
	\begin{figure}[h!]
		\centering
		\includegraphics{AchievedOccupancyVaryingRegisterCount}
		\caption{Varying register count chart example}
	\end{figure}


	\item Varying Shared Memory Usage

	Shows how varying the shared memory usage while holding other parameters constant would affect the theoretical occupancy. The circled point shows the current amount of shared memory per block and the current upper limit of active warps. If the chart's line goes higher than the circle, changing the amount of shared memory per block could increase occupancy without changing the other factors.
	
	\begin{figure}[h!]
		\centering
		\includegraphics{AchievedOccupancyVaryingSharedMemoryUsage}
		\caption{Varying shared memory usage chart example}
	\end{figure}

	\item Achieved Occupancy Per SM
	
	The achieved occupancy for each SM. The values reported are the average across all warp schedulers for the duration of the kernel execution. The line across all bars is the average, which is the number reported as Achieved Occupancy in the other tables.

	\begin{figure}[h!]
		\centering
		\includegraphics{AchievedOccupancyPerSm}
		\caption{Achieved o ccupancy per SM chart example}
	\end{figure}

\end{itemize}

\subsection{Instruction statistics}

The Instruction statistics experiment provides a first level triage for understanding the overall utilization of the target device when executing the kernel.

The global work distribution engine schedules thread blocks to Streaming Multiprocessors. A multiprocessor is considered to be active if it has at least one warp assigned to it. The total number of cycles a SM was active for the duration of the kernel execution is defined as Active Cycles.

Each multiprocessor exposes multiple warp schedulers that are able to execute at least one instruction per cycle. At every instruction issue time, each warp scheduler selects one warp that is able to make forward process from its assigned list of warps. For this selected warp the scheduler then issues either the next single instruction or the next two instructions.

A warp scheduler might need to issue an instruction multiple times to actually complete the execution for all 32 threads of a warp. The two primary reasons for this difference between Instructions Issued and Instructions Executed are: First, address divergence and bank conflicts on memory operations. Second, assembly instructions that can only be issued for a half-warp per cycle and thus need to be issued twice. Double floating-point instructions are the prime example for such instructions. As each executed instruction needs to be at least issued once, the following statement holds true in all cases:


\begin{center}
	Instructions Issued >= Instructions Executed
\end{center}


Issuing an instruction multiple times is also referred to as Instruction Replay. Each replay iteration takes away the ability to make forward progress by issuing new instructions on that warp scheduler. Also the compute resources required to process the instruction are consumed for every instruction replay. In short, the more instruction replay iterations are required the higher is the performance impact on the kernel execution.


\begin{itemize}	
	\item Instructions Per Clock (IPC)
	
	A z-ordered column graph showing the achieved instructions throughputs per SM for both, issued instructions and executed instructions. The theoretical maximum peak IPC is a device limit and defined by the compute capabilities of the target device. The y-axis is scaled to this peak value.
	\begin{itemize}	
		\item Issued IPC
		
		The average number of issued instructions per cycle accounting for every iteration of instruction replays. Optimal if as close as possible to the Executed IPC. Some assembly instructions require to be multi-issued. hence the instruction mix affects the definition of the optimal target for this metric.
	
		\item Executed IPC
		
		The average number of executed instructions per cycle. Higher numbers indicate more efficient usage of the available all resources. As each warp scheduler of a multiprocessor can execute instructions independently, a target goal of executing one instruction per cycle means executing on average with an IPC equal to the number of warp schedulers per SM. The maximum achievable target IPC for a kernel is dependent on the mixture of instructions executed.		
		
	\end{itemize}

	\begin{figure}[h!]
		\centering
		\includegraphics{InstructionStatisticsChartInstructionsPerClock}
		\caption{Instructions per clocks chart example}
	\end{figure}

	\item SM Activity
	Shows the percentage of time each multiprocessor was active during the duration of the kernel launch. A multiprocessor is considered to be active if at least one warp is currently assigned for execution. An SM can be inactive - even though the kernel grid is not yet completed - due to high workload imbalances. Such uneven balancing between the SMs can be caused by a few factors: Different execution times for the kernel blocks, variations between the number of scheduled blocks per SM, or a combination of the two.
	
	The observable result of a load imbalance are highly different activity values across the multiprocessors; simply caused by the fact that some SMs are still busy executing work, while others SMs already completed their share of work and stay idle as no more work items are left to be scheduled. This is typically referred to as a "tail effect". Small kernel grids with a low number of blocks are more likely to be affected by a tail effects.
	
		\begin{figure}[h!]
		\centering
		\includegraphics{InstructionStatisticsChartSmActivity}
		\caption{SM activity chart example}
	\end{figure}
	
	\item Instructions Per Warp (IPW)
	Shows the average executed instructions per warp for each multiprocessor. High variations in the IPW metric across the SMs indicate non-uniform workloads for the blocks of the kernel grid. While such imbalance does not necessarily have to result in low performance, IPW is very useful to understand the cause of variations in SM Activity.
	
	The most common code pattern to cause high variations in IPW is conditionally executed code blocks where the conditional expression is dependent on the block index. Examples include: special pre-processing or post-processing operations executed for a single block only, or costly detection and handling of edge conditions that are only triggered for some subset of the grid.
	
	\begin{figure}[h!]
		\centering
		\includegraphics{InstructionStatisticsChartInstructionsPerWarp}
		\caption{Instructions per warp chart example}
	\end{figure}
	
	
	\item Warps Launched
	Shows the total number of warps launched per multiprocessor for the executed kernel grid. Large differences in the number of warps launched per SM are most commonly the result of providing an insufficient amount of parallelism with the kernel grid. More specifically, the number of kernel blocks is too low to make good use of all available compute resources. A high variation in the number of warps launched is only a concern if the SM Activity is low on one or more SMs. In this case, partitioning workload that either result in less variance in execution duration per warp or in the execution of more thread blocks. Both cases will help the work distributor in dispatching the given work more evenly.
		
	\begin{figure}[h!]
		\centering
		\includegraphics{InstructionStatisticsChartWarpsLaunched}
		\caption{Warps launched chart example}
	\end{figure}	
\end{itemize}

\subsection{Branch statistics}

Flow control can have a serious impact on the efficiency of executing a kernel. Especially if a lot of flow control decisions are divergent, forcing the threads of a warp into very different control paths through the kernel code. The Branch Statistics experiment helps answering the question of how often flow control instructions were executed, how many of them were uniform versus divergent, and how much the flow control impacted the overall kernel execution performance.
A flow control instruction is considered to be divergent if it forces the threads of a warp to execute different execution paths. If this happens, the different execution paths must be serialized, since all of the threads of a warp share a program counter; this increases the total number of instructions executed for this warp. When all the different execution paths have completed, the threads converge back to the same execution path. Conditional expressions that evaluate to a uniform decision across all threads of a warp, do only execute the single, selected code path - consequently causing a lot less overhead. The ratio of executed uniform flow control decisions over all executed conditionals is defined as Branch Efficiency.

The actual performance impact caused by divergent flow control is proportional to the combination of how many different code paths need to be evaluated and how expensive the serialized code segments are. One way of capturing this is to track for all executed instructions how many of the threads in a warp were actually participating in the execution, i.e. how many threads were not predicated off. This is typically referred to as Control Flow Efficiency. By definition this is independent of the number of flow control decisions made, but rather states an upper limit of the utilization of the available compute resources due to flow control.


\begin{itemize}
	\item Efficiency
		
	The efficiency chart shows the two primary metrics for evaluating the impact of flow control.
	
	\begin{itemize}
		\item Branch Efficiency
		
		 States the ratio of uniform control flow decisions over all executed branch instructions. Shown per-SM (the bars) and averaged over all SMs (the Branch line). Higher values are better, as warps more often take a uniform code path. A value lower than 100\% is a necessary, but not sufficient indicator for a negative impact on the kernel execution performance, since the metric does not have any knowledge about the size of the code regions enclosed by the conditionals. For example, one divergent flow control decision out of ten executed branches may be negligible if it encloses very few lines of code only; but it may have a huge impact if it forced the warp to execute many different code paths with thousands of instructions.
		 
		 \item Control Flow Efficiency
		 
		  Defined as the ratio of active threads that are not predicated off over the maximum number of threads per warp for each executed instruction. Gets lower with fewer threads per warp being active per instruction; therefore serving as a metric for the efficiency in using the available processing units. Lower control flow efficiency can be caused by: Launching warps with less than 32 threads active. Terminating some threads in a warp earlier than others. Or executing instructions with only a subset of the threads enabled.
	\end{itemize}

	\begin{figure}[h!]
		\centering
		\includegraphics{BranchStatisticsChartEfficiency}
		\caption{Efficiency chart example}
	\end{figure}

	\item Branches per warp

	Shows the average count of executed branch instructions per warp per SM grouped by the outcome of the evaluation of the conditional statement. Useful to investigate the total amount of flow control instructions executed for the warps of the kernel grid.
	
	\begin{itemize}
		\item Not Taken / Taken 
		
		Average number of executed branch instructions with a uniform control flow decision per warp; that is all active threads of a warp either take or not take the branch
		
		\item Diverged 
		
		Average number of executed branch instruction per warp for which the conditional resulted in different outcomes across the threads of the warp. All code paths with at least one participating thread get executed sequentially. Lower numbers are better.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{BranchStatisticsChartBranchesPerWarp}
		\caption{Branches per warp chart example}
	\end{figure}


	\item Branches condition

	Shows the distribution of executed branches that were uniform versus divergent aggregated across all warps of the kernel grid.
	
	\begin{itemize}
		\item Not Taken / Taken 
		
		 Total number of executed branch instructions with a uniform control flow decision; that is all active threads of a warp either take or not take the branch.
		
		\item Diverged 
		
		Total number of executed branch instruction for which the conditional resulted in different outcomes across the threads of the warp. All code paths with at least one participating thread get executed sequentially. Lower numbers are better.
		
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{BranchStatisticsChartBranchCondition}
		\caption{Branch condition chart example}
	\end{figure}

\end{itemize}

\subsection{Issue efficiency}
The issue efficiency experiment provides information about the device's ability to issue the instructions. The key takeaway is the answer to the question if the device was able to issue instructions every cycle. Not being able to do so inevitably lowers the potential peak performance of the kernel.


\begin{itemize}
	\item Warps Per SM
	
	The metrics are reported as average values across the complete kernel execution for each individual SM of the target device. The y-Axis is scaled to the device limit.
	
	
	\begin{itemize}
		\item Active Warps  
		
		A warp is active from the time it is scheduled on a multiprocessor until it completes the last instruction. Each warp scheduler maintains its own list of assigned active warps. This assignment of warps to the schedulers is done once at the time a warp becomes active and is valid for the lifetime of the warp. 
		
		\item Eligible Warps 
		
		An active warp is considered eligible if it is able to issue the next instruction. Each warp scheduler will select the next warp to issue an instruction from the pool of eligible warps. Warps that are not eligible will report an Issue Stall Reason. The target is to have at least one eligible warp per scheduler per cycle.
		
		\item Theoretical Occupancy
		
		The theoretical occupancy acts as upper limit to active warps and consequently also eligible warps per SM. It is defined by the execution configuration of the kernel launch.
		
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{IssueEfficiencyChartWarpsPerSm}
		\caption{Warps per SM chart example}
	\end{figure}

	\item Warp Issue Efficiency
	
	The metrics are reported as average values across the complete kernel execution for each individual SM of the target device. The y-Axis is scaled to the device limit.
	
	
	\begin{itemize}
		\item Active Warps  
		
		On every clock cycle, a warp scheduler tries to issue an instruction from one of its warps. When a warp issues an instruction, it takes at least a few cycles before it becomes eligible to issue again, so many warps should be active on a warp scheduler to ensure it can issue an instruction from some warp on every cycle. In this experiment, the profiler counts whether an instruction was issued or not for each clock cycle on each warp scheduler. The Warp Issue Efficiency chart shows the average across all warp schedulers over the duration of kernel execution. 
		
		\item No Eligible  
		
		The number of cycles that a warp scheduler had no eligible warps to select from and therefore did not issue an instruction. The lower the percentage of cycles with no eligible warp the more efficient the code runs on the target device.
		
		\item One or More Eligible
		
		The number of cycles that a warp scheduler had at least one eligible warps to select from. This metric is equal to total number of cycles an instruction was issued summed across all warp schedulers. Better if the value is higher with a target of getting close to 100\%.
		
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{IssueEfficiencyChartWarpIssueEfficiency}
		\caption{Warp Issue Efficiency chart example}
	\end{figure}




	\item Issue Stall Reasons

	The issue stall reasons capture why an active warp is not eligible. On devices of compute capability 3.0 and higher, every stalled warp increments its most critical stall reason by one on every cycle. The sum of the stall reasons, hence increment per multiprocessor per cycle, by a value between zero (if all warps are eligible) and the number of active warps (if all warps are stalled). The update of the stall reason counters occurs for all stalled warps independent of being able to issue an instruction that cycle or not.
	
	
	\begin{itemize}
		\item Pipeline Busy   
		
		The compute resources required by the instruction are not yet available.
		
		\item Texture 
		
		The texture subsystem is fully utilized, or has too many outstanding requests
		
		\item Constant 
		
		A constant load is blocked due to a miss in the constants cache.
		
		\item Instruction Fetch  
		
		The next assembly instruction has not yet been fetched.
		
		\item Memory Throttle 
		
		A large number of pending memory operations prevent further forward progress. These can be reduced by combining several memory transactions into one.
		
		\item Memory Dependency  
		
		A load/store cannot be made because the required resources are not available or are fully utilized, or too many requests of a given type are outstanding. Memory dependency stalls can potentially be reduced by optimizing memory alignment and access patterns.
		
		\item Synchronization  
		
		 The warp is blocked at a \_syncthreads() call.
				
		\item Execution Dependency 
		
		An input required by the instruction is not yet available. Execution dependency stalls can potentially be reduced by increasing instruction-level parallelism.
						
							
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{analysis_issue_stall_reasons}
		\caption{Issue Stall Reason chart example}
	\end{figure}
		
\end{itemize}

\subsection{Pipe utilization}

Each Streaming Multiprocessor (SM) of a CUDA device features numerous hardware units that are specialized in performing specific task. At the chip level those units provide execution pipelines to which the warp schedulers dispatch instructions to. For example, texture units provide the ability to execute texture fetches and perform texture filtering. Load/Store units fetch and save data to memory. Understanding the utilization of those pipelines and knowing how close they are to the peak performance of the target device are key information for analyzing the efficiency of executing a kernel; and also allows to identify performance bottlenecks caused by oversubscribing to a certain type of pipeline.

Pipeline Utilization metrics report the observed utilization for each pipeline at runtime. High pipeline utilization states that the corresponding compute resources were used heavily and kept busy often during the execution of the kernel. Low values indicate that the pipeline is not frequently used and resources were idle. The results for individual pipelines are independent of each other; summing up two or more pipeline utilization percentages does not result in a meaningful value. As the pipeline metrics are reported as an average over the duration of the kernel launch, a low value does not necessarily rule out that the pipeline was a bottleneck at some point in time during the kernel execution.

\begin{itemize}
	\item Pipe Utilization Chart
	
	Shows the average utilization of the four major logical pipelines of the SMs during the execution of the kernel. Useful for investigating if a pipeline is oversubscribed and therefore is limiting the kernel's performance. Also helpful to estimate if adding more work will scale well or if a pipeline limit will be hit. In this context adding more work may refer to adding more arithmetic workload (for example by increasing the accuracy of some calculations), increasing the number of memory operations (including introducing register spilling), or increasing the number of active warps per SM with the goal of improving instruction latency hiding.
	
	
	\begin{itemize}
		\item Load / Store  
		
		Covers all issued instructions that trigger a request to the memory system of the target device - excluding texture operations. Accounts for load and store operations to global, local, shared memory as well as any atomic operation. Also includes register spills. Devices of compute capability 3.5 and higher support loading global memory through the read-only data cache (LDG); those operations do not contribute to the load/store group, but are accounted for in the texture pipeline utilization instead. 
		
		\item Texture 
		
		Covers all issued instructions that perform a texture fetch and, for devices of compute capability 3.5 and higher, global memory loads via the  read-only data cache.
		
		\item Control Flow
		
		Covers all issued instructions that can have an effect on the control flow, such as branch instructions (BRA,BRX), jump instructions (JMP,JMX), function calls (CAL,JCAL), loop control instructions (BRK,CONT), return instructions (RET), program termination (EXIT), and barrier synchronization (BAR).
		
		\item Arithmetic 
		
		Covers all issued floating point instructions, integer instructions, conversion operations, and movement instructions.
		
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{PipeUtilizationChart}
		\caption{Pipe Utilization chart example}
	\end{figure}
	
	\item Arithmetic Workload
	
	Provides the distribution of estimated costs for numerous classes of arithmetic instructions. The cost model is based on the issue count weighted by the reciprocal of the corresponding instruction throughput. 

	
	\begin{itemize}
		\item FP16 
		
		Estimated workload for all 16-bit floating-point add (HADD), multiply (HMUL), multiply-add (HFMA) instructions.
		
		\item FP32 
		
		Estimated workload for all 32-bit floating-point add (FADD), multiply (FMUL), multiply-add (FMAD) instructions.
		
		\item FP64 
		
		Estimated workload for all 64-bit floating-point add (DADD), multiply (DMUL), multiply-add (DMAD) instructions.
		
		\item FP32 (Special) 
		
		Estimated workload for all 32-bit floating-point reciprocal (RCP), reciprocal square root (RSQ), base-2 logarithm (LG2), base 2 exponential (EX2), sine (SIN), cosine (COS) instructions.
		
		\item I32 (Add) 
		
		Estimated workload for all 32-bit integer add (IADD), extended-precision add, subtract, extended-precision subtract, minimum (IMNMX), maximum instructions.
		
		\item I32 (Mul) 
		
		Estimated workload for all 32-bit integer multiply (IMUL), multiply-add (IMAD), extended-precision multiply-add, sum of absolute difference (ISAD), population count (POPC), count of leading zeros, most significant non-sign bit (FLO).
		
		\item I32 (Shift) 
		
		Estimated workload for all 32-bit integer shift left (SHL), shift right (SHR), funnel shift (SHF) instructions.
		
		\item Cmp/Min/Max 
		
		Estimated workload for all comparison operations
		
		\item I32 (Bitfield/Rev) 
		
		Estimated workload for all 32-bit integer bit reverse, bit field extract (BFE), and bit field insert (BFI) instructions
		
		\item I32 (Bitwise Logic) 
		
		Estimated workload for all logical operations (LOP).
		
		\item Warp Shuffle
		
		Estimated workload for all warp shuffle (SHFL) instructions.
		
		\item Video SIMD 
		
		Estimated workload for all video vector instructions
		
		\item Conv (From I8/I16 to I32)
		
		Estimated workload for all type conversions from 8-bit and 16-bit integer to 32-bit types (subset of I2I).
		
		\item Conv (To/From FP64) 
		
		Estimated workload for all type conversions from and to 64-bit types (subset of I2F, F2I, and F2F).
		
		\item Conv (All Other) 
		
		Estimated workload for all all other type conversions (remaining subset of I2I, I2F, F2I, and F2F).
		
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics{PipeUtilization_ArithmeticWorkload}
		\caption{Arithmetic Workload chart example}
	\end{figure}
		
\end{itemize}

\subsection{Memory statistics}

\begin{itemize}
	\item Overview Chart
	
	Shows a summary view of the memory hierarchy of the CUDA programming model. Key metrics are reported for the areas that were covered by memory experiments during the data collection. The nodes in the diagram depict either a logical memory space (global, local, shared, ...) or an actual hardware unit on the chip (caches, shared memory, device memory). For the various caches the reported percentage number states the cache hit rate; that is the ratio of requests that could be served with data locally available to the cache over all requests made. Requests that hit data in the cache are served much faster than requests that miss the cache; missed data needs to be fetched from another layer of the memory hierarchy.
	
	Links between the nodes in the diagram depict the data paths between the SMs to the memory spaces into the memory system.
	
	\begin{figure}[H]
		\centering
		\includegraphics{MemoryStatisticsChart}
		\caption{Memory Statistics chart example}
	\end{figure}
s
	\item Shared memory Chart
	
	The number of Load/Store Requests equals the amount of shared memory instructions executed. When a warp executes an instruction that accesses shared memory, it resolves the bank conflicts. Each bank conflict forces a new memory transaction. The more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly. Each memory transaction also requires the assembly instruction to be issued again; causing instruction replays if more than one transaction is required to fulfill the request of a warp.
	
	The Transactions Per Request chart shows the average number of shared memory transactions required per executed shared memory instruction, separately for load and store operations. Lower numbers are better; the target for a single shared memory operation on a full warp is 1 transaction for both, a 4byte access and an 8byte access, and 2 transactions for a 16byte access."
	
	\begin{figure}[H]
		\centering
		\includegraphics{MemoryStatisticsSharedChart}
		\caption{Shared Memory Statistics chart example}
	\end{figure}

\end{itemize}


\section{Equals}
Figure \ref{fig:equals} presents execution times of the function Equals in $\mu$s based on input's bitwise length. Below are charts generated by NSight.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0 2.5cm 0 2.5cm},clip]{equals.pdf}.
	\caption{Execution time of function Equals in $\mu$s depending on input's bitwise length}
	\label{fig:equals}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_occupancy}.
	\caption{Occupancy statistics of Equals function}
	\label{fig:equals_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{equals_occupancy_charts}.
	\caption{Occupancy charts of Equals function}
	\label{fig:equals_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_instructions}.
	\caption{Instruction statistics of Equals function}
	\label{fig:equals_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_branch}.
	\caption{Branch statistics of Equals function}
	\label{fig:equals_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_issue}.
	\caption{Issue efficiency of Equals function}
	\label{fig:equals_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_pipe}.
	\caption{Pipe utilization of Equals function}
	\label{fig:equals_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_memory}.
	\caption{Memory overview of Equals function}
	\label{fig:equals_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{equals_shared}.
	\caption{Shared memory chart of Equals function}
	\label{fig:equals_shared}
\end{figure}

 Analyzing figure \ref{fig:equals} we can see the trend line showing the average time actually drops as the input's length rises. This is probably caused by caching techniques implemented on the device. It is safe to state the function runs in constant time, and is resistant to timing attacks. 

\section{Compare}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0 2.5cm 0 2.5cm},clip]{compare.pdf}.
	\caption{Execution time of function Compare in $\mu$s depending on input's bitwise length}
	\label{fig:compare}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_occupancy}.
	\caption{Occupancy statistics of Compare function}
	\label{fig:compare_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{compare_occupancy_charts}.
	\caption{Occupancy charts of Compare function}
	\label{fig:compare_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{compare_instructions}.
	\caption{Instruction statistics of Compare function}
	\label{fig:compare_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_branch}.
	\caption{Branch statistics of Compare function}
	\label{fig:compare_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_issue}.
	\caption{Issue efficiency of Compare function}
	\label{fig:compare_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_pipe}.
	\caption{Pipe utilization of Compare function}
	\label{fig:compare_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_memory}.
	\caption{Memory overview of Compare function}
	\label{fig:compare_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{compare_shared}.
	\caption{Shared memory chart of Compare function}
	\label{fig:compare_shared}
\end{figure}


\section{Bit length}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{bitlength.pdf}.
	\caption{Execution time of function getBitLength in $\mu$s, depending on input's bitwise length}
	\label{fig:bitlength}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_occupancy}.
	\caption{Occupancy statistics of getBitLength function}
	\label{fig:bitlength_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{bitlength_occupancy_charts}.
	\caption{Occupancy charts of getBitLength function}
	\label{fig:bitlength_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{bitlength_instructions}.
	\caption{Instruction statistics of getBitLength function}
	\label{fig:bitlength_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_branch}.
	\caption{Branch statistics of getBitLength function}
	\label{fig:bitlength_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_issue}.
	\caption{Issue efficiency of getBitLength function}
	\label{fig:bitlength_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_pipe}.
	\caption{Pipe utilization of getBitLength function}
	\label{fig:bitlength_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_memory}.
	\caption{Memory overview of getBitLength function}
	\label{fig:bitlength_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{bitlength_shared}.
	\caption{Shared memory chart of getBitLength function}
	\label{fig:bitlength_shared}
\end{figure}

\section{Left shift}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{leftshift.pdf}.
	\caption{Execution time of function Left shift in $\mu$s, depending on number of bits to shift}
	\label{fig:leftshift}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_occupancy}.
	\caption{Occupancy statistics of Left shift function}
	\label{fig:leftshift_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{leftshift_occupancy_charts}.
	\caption{Occupancy charts of Left shift function}
	\label{fig:leftshift_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{leftshift_instructions}.
	\caption{Instruction statistics of Left shift function}
	\label{fig:leftshift_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_branch}.
	\caption{Branch statistics of Left shift function}
	\label{fig:leftshift_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_issue}.
	\caption{Issue efficiency of Left shift function}
	\label{fig:leftshift_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_pipe}.
	\caption{Pipe utilization of Left shift function}
	\label{fig:leftshift_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_memory}.
	\caption{Memory overview of Left shift function}
	\label{fig:leftshift_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{leftshift_shared}.
	\caption{Shared memory chart of Left shift function}
	\label{fig:leftshift_shared}
\end{figure}

\section{Right shift}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{rightshift.pdf}.
	\caption{Execution time of function Right shift in $\mu$s, depending on number of bits to shift}
	\label{fig:rightshift}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_occupancy}.
	\caption{Occupancy statistics of Right shift function}
	\label{fig:rightshift_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{rightshift_occupancy_charts}.
	\caption{Occupancy charts of Right shift function}
	\label{fig:rightshift_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{rightshift_instructions}.
	\caption{Instruction statistics of Right shift function}
	\label{fig:rightshift_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_branch}.
	\caption{Branch statistics of Right shift function}
	\label{fig:rightshift_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_issue}.
	\caption{Issue efficiency of Right shift function}
	\label{fig:rightshift_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_pipe}.
	\caption{Pipe utilization of Right shift function}
	\label{fig:rightshift_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_memory}.
	\caption{Memory overview of Right shift function}
	\label{fig:rightshift_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{rightshift_shared}.
	\caption{Shared memory chart of Right shift function}
	\label{fig:rightshift_shared}
\end{figure}

\section{Add}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{add.pdf}.
	\caption{Execution time of function Add in $\mu$s depending on input's bitwise length}
	\label{fig:add}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_occupancy}.
	\caption{Occupancy statistics of Add function}
	\label{fig:add_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{add_occupancy_charts}.
	\caption{Occupancy charts of Add function}
	\label{fig:add_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{add_instructions}.
	\caption{Instruction statistics of Add function}
	\label{fig:add_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_branch}.
	\caption{Branch statistics of Add function}
	\label{fig:add_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_issue}.
	\caption{Issue efficiency of Add function}
	\label{fig:add_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_pipe}.
	\caption{Pipe utilization of Add function}
	\label{fig:add_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_memory}.
	\caption{Memory overview of Add function}
	\label{fig:add_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{add_shared}.
	\caption{Shared memory chart of Add function}
	\label{fig:add_shared}
\end{figure}

\section{Subtract}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{subtract.pdf}.
	\caption{Execution time of function Subtract in $\mu$s depending on input's bitwise length}
	\label{fig:subtract}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_occupancy}.
	\caption{Occupancy statistics of Subtract function}
	\label{fig:subtract_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{subtract_occupancy_charts}.
	\caption{Occupancy charts of Subtract function}
	\label{fig:subtract_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{subtract_instructions}.
	\caption{Instruction statistics of Subtract function}
	\label{fig:subtract_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_branch}.
	\caption{Branch statistics of Subtract function}
	\label{fig:subtract_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_issue}.
	\caption{Issue efficiency of Subtract function}
	\label{fig:subtract_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_pipe}.
	\caption{Pipe utilization of Subtract function}
	\label{fig:subtract_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_memory}.
	\caption{Memory overview of Subtract function}
	\label{fig:subtract_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{subtract_shared}.
	\caption{Shared memory chart of Subtract function}
	\label{fig:subtract_shared}
\end{figure}

\section{Multiply}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{multiply.pdf}.
	\caption{Execution time of function Multiply in $\mu$s depending on input's bitwise length}
	\label{fig:multiply}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_occupancy}.
	\caption{Occupancy statistics of Multiply function}
	\label{fig:multiply_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{mull_occupancy_charts}.
	\caption{Occupancy charts of Multiply function}
	\label{fig:multiply_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{mull_instructions}.
	\caption{Instruction statistics of Multiply function}
	\label{fig:multiply_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_branch}.
	\caption{Branch statistics of Multiply function}
	\label{fig:multiply_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_issue}.
	\caption{Issue efficiency of Multiply function}
	\label{fig:multiply_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_pipe}.
	\caption{Pipe utilization of Multiply function}
	\label{fig:multiply_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_memory}.
	\caption{Memory overview of Multiply function}
	\label{fig:multiply_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mull_shared}.
	\caption{Shared memory chart of Multiply function}
	\label{fig:multiply_shared}
\end{figure}


\section{Modulo reduction}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{mod.pdf}.
	\caption{Execution time of function Modulo reduction in $\mu$s depending on input's bitwise length}
	\label{fig:mod}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{mod_diff.pdf}.
	\caption{Execution time of function Modulo reduction in ms depending on inputs' bitwise lengths differences}
	\label{fig:mod_diff}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_occupancy}.
	\caption{Occupancy statistics of Modulo reduction function}
	\label{fig:mod_occupancy}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{mod_occupancy_charts}.
	\caption{Occupancy charts of Modulo reduction function}
	\label{fig:mod_occupancy_charts}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, height=26cm,keepaspectratio]{mod_instructions}.
	\caption{Instruction statistics of Modulo reduction function}
	\label{fig:mod_instructions}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_branch}.
	\caption{Branch statistics of Modulo reduction function}
	\label{fig:mod_branch}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_issue}.
	\caption{Issue efficiency of Modulo reduction function}
	\label{fig:mod_issue}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_pipe}.
	\caption{Pipe utilization of Modulo reduction function}
	\label{fig:mod_pipe}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_memory}.
	\caption{Memory overview of Modulo reduction function}
	\label{fig:mod_memory}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{mod_shared}.
	\caption{Shared memory chart of Modulo reduction function}
	\label{fig:mod_shared}
\end{figure}

\section{Multiply modulo}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{mull_mod.pdf}.
	\caption{Execution time of function MultiplyMod in $\mu$s depending on input's bitwise length}
	\label{fig:mull_mod}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{mull_mod_diff.pdf}.
	\caption{Execution time of function MultiplyMod in ms depending on inputs' bitwise lengths differences}
	\label{fig:mull_mod_diff}
\end{figure}

\section{Power modulo}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{powmod.pdf}.
	\caption{Execution time of function Power modulo in ms depending on exponent's bitwise length}
	\label{fig:powmod}
\end{figure}

\section{RSA encrypt}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,trim={0.5cm 2.8cm 0.4cm 2.8cm},clip]{rsa.pdf}.
	\caption{Execution time of function encrypt in ms depending on message's bitwise length}
	\label{fig:rsa}
\end{figure}


\addcontentsline{toc}{chapter}{References}
\bibliography{bibliography}
 \listoffigures
 \listoftables
\end{document}

